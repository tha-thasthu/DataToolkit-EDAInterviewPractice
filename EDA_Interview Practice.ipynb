{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is Exploratory Data Analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exploratory Data Analysis refers to the critical process of performing initial investigations on data so as to discover patterns,to spot anomalies,to test hypothesis and to check assumptions with the help of summary statistics and graphical representations.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In EDA, we try to understand our data by summarizing the key concepts.\n",
    "This summarizing can be done numerically or visually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps you would want to take are:\n",
    "* **Identification of important varaibles**: Given a raw data set, we first identify which variables to consider in the context of the problem and which features we intend to make predictions for\n",
    "*  We need to identify the datatypes of each variables, and whether the variables are categorical or continous\n",
    "* **Univariate Analysis:** We will proceed with Univariate analysis to understand each column in the data. The way in which the univariate analysis is done will depend on whether the column we analyzed is numeric or categorical. \n",
    "* **Bivariate Analysis:** Also, in order to understand the data further, we want to know how the variables affect each other, this can be done using Bivariate analysis, in which we try to find the relationship between any two variables present in the dataset\n",
    "* **Data Visualization:** We may also want to visualize the data using appropriate plots for better understanding of the dataset\n",
    "* **Handling missing values and outliers:** We then handle missing values and outliers if present in the data\n",
    "* **Transform original variables:** We may also want to transform the variables. For e.g. in case we are going to build some model like Linear Regression, to make predictions using this data we need to keep in mind that such models will only accept the data in numerical format. Hence, we need to tranform any column having non-numeric values to numeric values before proceeding\n",
    "* **Derive new variables:** Also we may need to create new variables from the existing ones. In case of a column consisting of dates, like 01-Jan-2020, we may want 3 separate columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Are missing values always due to chance alone?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No. It's not necessary that data with missing values are always due to chance alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Estimate average income of the population thruough a questionnaire survey:** \n",
    "* We wish to find the average income of a population based on questionnaries filled by them. We have distributed some questionnaire in which people will share information related to their income. \n",
    "* Let's say there where 1000 people who filled the questionnaire. If 20 of the questionnaires where mis-placed, then the income information missing for these 20 people is not missing due to some other factor, but its missing purely due to chance, something we refer to as **MCAR**\n",
    "* Let's subset the questionnaires into each person's designation. Here certain observations where designation is manager there are more missing values. It may so happen that managers are less likely to share information related to the income assuming people with higher salary generally donot want to reveal their income in surveys. Thus, there may be more missing data for certain groups than other groups. Then **MAR**\n",
    "* People may not want to share their income information. In case, if they are earning less they may leave the value blank in the survey. In this case, the data is not missing at random, as people consider certain information sensitive. More sensitive the issue, the less likely people are going to give information. This means, there can be an underlying reason for the missing data. This is referred to as **MNAR**.\n",
    "\n",
    "**Possible types of missing values**\n",
    "* **Missing Completely at Random — (MCAR)**: If the probability of some value being missing is same for all observations in a column, then the data is said to be MCAR\n",
    "* **Missing at Random — (MAR)**: If the probability of being missing is the same for all points only within the group defined by the observed data, then the data are missing at random. \n",
    "* **Missing Not at Random — (MNAR)**: It is not necessary that the data is always missing due to chance alone, there may be some underlying factor resulting in the data missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. What is your approach to handle missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop the observations**<br>\n",
    "One of the ways of handling missing values is to drop all those observations those are missing. For this we need to be sure that the data is missing due to chance alone, i.e. MCAR.\n",
    "\n",
    "**Replace or impute the values with mean, median, etc**<br>\n",
    "Replace or impute the values with mean, median for Numeric data.<br>\n",
    "Replace or impute the values with mode for Categorical data.<br>\n",
    "Essentially, we replace the missing values with measures of the central tendency for each column in the dataset.\n",
    "\n",
    "**Create another level for missing categorical data**<br>\n",
    "Another unique value which replaces all missing values in that column.\n",
    "\n",
    "**Run predictive models to impute missing data**<br>\n",
    "We treat the missing columns as the target column. The training data will be the rows in which the target column has values and the test data will comprise the rows where the target column does not have values. We can now build a model using the training data and use this model to predict the missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dose a correlation coefficient of 0 between two numeric variables mean no relationship between them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is correlation coefficient\n",
    "**Measures the linear relationship of 2 variables**\n",
    "This can only tell us about the linear relationship between variables and how strongly they are related to each other. Non-linear relationships are not identified by the correlation coefficient.<br>\n",
    "\n",
    "**Ranges between -1 to +1**\n",
    "The value of correlation coefficient ranges from -1 to +1<br>\n",
    "+1 implies a strong positive linear relationship<br>\n",
    "-1 implies a strong negative linear relationship<br>\n",
    "0 implies no linear relationship<br>\n",
    "* If a variable has a **correlation coefficient of +0.9** it means that the variables are strongly positively linearly correlated.\n",
    "* If a variable has a **correlation coefficient of -0.9** it means that the variables are strongly negatively linearly correlated.\n",
    "* If a variables has a **correlation coefficient closer to 0** doesnot necessarly mean that there is no relationship between the two variables, all it means that there is no linear relationship between the two variables. There might be strong **non-linear relationship** between the those two variables. For e.g. the consumption of fuel increases till the speed is under 60kms/hr but gradually decreases when the speed goes beyond 80+kms/hr."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. It is always observed that more the number of ice creams sold in a city, higher are the number of murders in that city i.e. there seems to be a high correlation between the number of ice creams sold and the number of murders committed in a city. Can we say that because of the higher ice cream sales in the city there are higher murders?\n",
    "\n",
    "This is an example of why correlation between two variables doesnot imply causation. <br>\n",
    "\n",
    "**Correlation seeks to determine a linear relationship**\n",
    "Correlation tells us how a pair of numeric variables are linearly related. It does not give the reason or cause behind the relation but only tells us that the linear relationship exists between those two variables.\n",
    "\n",
    "**Causation implies a cause and effect relationship**\n",
    "Causation implies that the change in one variable has resulted in a change in an another variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. How do we confirm causation between the variables?\n",
    "\n",
    "There are two ways using which causality can be confirmed between two variables:\n",
    "* Experimental Studies\n",
    "* Observational Studies\n",
    "\n",
    "#### Experimental Studies\n",
    "* Assign people to two groups\n",
    "* One group is given treatment\n",
    "* Other group is not given treatment\n",
    "This is not a good practically possible always.\n",
    "\n",
    "#### Observational Studies\n",
    "* Measure treatment effect without any intervention\n",
    "\n",
    "**Examples**\n",
    "* Grouping people in terms of smokers or not and run tests to check on their health\n",
    "* Check if social media usage keeps people happier. So a group of 2 folks, one can use social media and another cannot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Given a pair of numeric variables with 'n' observations, how would you visualize it? In what case would a line plot not be suitable?\n",
    "\n",
    "#### Data Visualization\n",
    "* Use scatter plot for 2 numeric variables\n",
    "* Use line plot \n",
    " * Display points connected by a line\n",
    " * **Data should be sequential**\n",
    " * one variable is some function of time, like data from census or temperature over a period of time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. While dealing with multi-dimensional data, how can we visualize more than two variables (say 3 or 4 variables) in two dimensions without using any dimensionality reduction techniques?\n",
    "\n",
    "Visualization is done so that the person who views the plot can understand the patterns we wish to convey. It should not be too complex to understand and neither it should be so simple that the relevant points are not convied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. What are the different ways in which outliers can be handled?\n",
    "\n",
    "#### What is an Outlier\n",
    "An outlier is an observation that lies an abnormal distance from other values in a random sample from a population.<br>\n",
    "It can be due to a mistake or variance in the data.<br>\n",
    "Example:\n",
    "* Likes on the YouTube videos, can be very high for some videos and none of few videos as well. In this case outliers cannot be neglected\n",
    "* If an age of a person is recorded as negative value, then this for sure is a mistake and we need to remove or rectify this data\n",
    "\n",
    "#### Handling Outliers\n",
    "* **Drop the observations** in case its an measurement error. But we can rarely be sure that this has been recorded due to measurement error. Deleting an observation just because it is an outlier is a bad idea, since it may be the most interesting one\n",
    "* **Replace the  extreme values with a fixed value**. i.e. use Trimming at both ends to remove outliers. We may replace bottom 5% of values with the minimum value present in the 5th Percentile & we can also remove top 5% of values and replace it with maximum value present in the 95th Percentile\n",
    "* **Impute the outliers with the measure of central tendency: mean, median, etc.** just like we handle missing data\n",
    "* **Run models for predicting outliers obervations**\n",
    "* **Transforming the variables** by taking a natural log reduces the variation caused by the extreme values\n",
    "* **Binning continous variables** into groups as well which allows us to deal with the outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. What would you prefer as a measure of variability of your data - \"standard deviation\" or the \"interquartile range\" (IQR)?\n",
    "\n",
    "#### Interquartile Range (IQR)\n",
    "* Indicates the intendent to which the central 50% of values within the dataset are spread\n",
    "* The numbers used to calc IQR are not the extreme values but the 25th & 75th Percentile and thus, this measure is not affected by the extreme values and can be considered as a **robust measure**\n",
    "\n",
    "#### Standard Deviation\n",
    "* Its a measure that indicates how far each value is from the mean\n",
    "* Effectively it indicates how tightly the values in the column are bunched around the mean values \n",
    "* Since SD considers each value in the column when measuring the variablity of the data in that column, this measure can get affected quite a bit by extreme values and hence can be considered as a **sensitive measure**\n",
    "\n",
    "#### Now which measure to use when computing variability?\n",
    "* IQR is preferred in data containing extreme values\n",
    "* If extreme values are not present, use standard deviation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
